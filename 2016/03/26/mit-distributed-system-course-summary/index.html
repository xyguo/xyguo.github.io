<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Summary for MIT 6.824- Course projects | Coupon Collector</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Several projects including MapReduce, Primary/Backup Key/Value Service, Paxos-based Key/Value Service, Sharded Key/Value Service, Persistence. Mainly about the problems encountered during implementing">
<meta property="og:type" content="article">
<meta property="og:title" content="Summary for MIT 6.824- Course projects">
<meta property="og:url" content="http://yoursite.com/2016/03/26/mit-distributed-system-course-summary/index.html">
<meta property="og:site_name" content="Coupon Collector">
<meta property="og:description" content="Several projects including MapReduce, Primary/Backup Key/Value Service, Paxos-based Key/Value Service, Sharded Key/Value Service, Persistence. Mainly about the problems encountered during implementing">
<meta property="og:updated_time" content="2016-06-20T07:24:54.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Summary for MIT 6.824- Course projects">
<meta name="twitter:description" content="Several projects including MapReduce, Primary/Backup Key/Value Service, Paxos-based Key/Value Service, Sharded Key/Value Service, Persistence. Mainly about the problems encountered during implementing">
  
    <link rel="alternate" href="/atom.xml" title="Coupon Collector" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Coupon Collector</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">miscellaneous notes</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
          <a class="main-nav-link" href="/links">Links</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-mit-distributed-system-course-summary" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/03/26/mit-distributed-system-course-summary/" class="article-date">
  <time datetime="2016-03-26T02:29:22.000Z" itemprop="datePublished">2016-03-26</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/programming/">programming</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Summary for MIT 6.824- Course projects
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>从去年国庆节的时候开始刷MIT的分布式系统课程，到3月初的时候终于断断续续地把作业都做完了。刷了这个课才终于知道为什么美帝的小伙伴们一个学期最多就选个三门课了——作业又多又难，选多了铁定要跪。就比如这门6.824，从头至尾读了几十篇论文，写的代码比我之前一个学期的代码总量都多。反观自己在国内选修的一些课程，大部分都水的不能看，纯粹混学分，一个学期可以修四五门，简直药丸。</p>
<p>这篇文章主要记录了我在做作业时碰到的一些坑。可能还有一些坑我自己都没发现，毕竟分布式系统设计中有很多subtle的地方，尤其是涉及某些分布式一致性算法时（没错我就是在吐槽Paxos），各个节点间应交换哪些信息、针对不同的信息应做何种处理都很有门道，绝对不能靠想当然，否则你总会发现自己莫名其妙的就破坏了一致性。另外调试这种程序基本只能靠把日志打印出来一条条分析，若对算法没有非常清晰的认识、知道什么该发生什么不该发生，那从日志里也不可能看出什么有用的东西。</p>
<h1 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h1><h2 id="Preliminary-of-MapReduce"><a href="#Preliminary-of-MapReduce" class="headerlink" title="Preliminary of MapReduce"></a>Preliminary of MapReduce</h2><ul>
<li>三个阶段：Map–&gt;Merge–&gt;Reduce</li>
<li>Map在Mapper上进行， Merge和Reduce则都在Reducer上进行</li>
<li>Reducer的数目不能太多，因为Mapper生成的中间文件的数目是#Mapper * #Reducer</li>
<li>所有Mapper<strong>共用</strong>一个hash function来把不同的key映射到其对应的Reducer，因此保证最后相同的key会集中到同一个Reducer上</li>
</ul>
<h2 id="Main-problem-the-RunMaster-function-in-master-go"><a href="#Main-problem-the-RunMaster-function-in-master-go" class="headerlink" title="Main problem: the RunMaster() function in master.go"></a>Main problem: the <code>RunMaster()</code> function in <code>master.go</code></h2><p>这个函数的主要作用是分配、调度任务，在用户指定了Map任务的数目<code>nMap</code>和Reduce任务的数目<code>nReduce</code>后，分配足够的worker来完成任务。注意这里我说的是“任务的数目”，而不是“Worker”的数目：因为实际上<code>nMap</code>和<code>nReduce</code>可以任意指定，这个只影响生成的中间文件的数目，然而系统中worker的总数是固定的（且完全可能小于<code>nMap</code>和<code>nReduce</code>），这就意味着<strong>在某个worker完成一项任务后，需要调度它去完成剩余的任务</strong>，worker的角色只由它分配到的任务决定，一个worker可以先是Mapper然后又变成了Reducer。相比于实际中的MapReduce系统，这里没有考虑“Move computation to data”的就近原则。</p>
<p>主要的实现思想：在Map阶段，<code>RunMaster()</code>函数持续地从<code>mr.availableWorkers</code>这个channel中获取idle worker的名字，然后让它去完成一个Map任务，在worker完成任务后，由把它重新加到<code>mr.availableWorkers</code>中去。上述操作通过创建一个goroutine来完成，因此这个调度过程是并发的。Reduce步骤的方法类似。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Handle out Map jobs in parallel</span></span><br><span class="line">taskDoneChannel := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>)</span><br><span class="line"><span class="keyword">for</span> m := <span class="number">0</span>; m &lt; mr.nMap; m++ &#123;</span><br><span class="line">	<span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">(n <span class="keyword">int</span>)</span></span> &#123;</span><br><span class="line">		jobArgs := DoJobArgs&#123; ... &#125;</span><br><span class="line">		<span class="keyword">var</span> reply DoJobReply</span><br><span class="line"></span><br><span class="line">		ok := <span class="literal">false</span></span><br><span class="line">		worker := &lt;-mr.availableWorkers</span><br><span class="line">		ok = call(worker, <span class="string">"Worker.DoJob"</span>, &amp;jobArgs, &amp;reply)</span><br><span class="line">		<span class="keyword">for</span> ok != <span class="literal">true</span> &#123;</span><br><span class="line">			<span class="comment">// if worker failed, set it available and switch to another one</span></span><br><span class="line">			<span class="keyword">go</span> setAvailable(worker)</span><br><span class="line">			worker = &lt;-mr.availableWorkers</span><br><span class="line">			ok = call(worker, <span class="string">"Worker.DoJob"</span>, &amp;jobArgs, &amp;reply)</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// if worker succeed, set it available</span></span><br><span class="line">		<span class="keyword">go</span> setAvailable(worker)</span><br><span class="line">		taskDoneChannel &lt;- <span class="number">1</span></span><br><span class="line">	&#125;(m)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ensure all map tasks being completed</span></span><br><span class="line"><span class="keyword">for</span> m := <span class="number">0</span>; m &lt; mr.nMap; m++ &#123;</span><br><span class="line">	&lt;-taskDoneChannel</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="Primary-Backup-Key-Value-Service"><a href="#Primary-Backup-Key-Value-Service" class="headerlink" title="Primary/Backup Key/Value Service"></a>Primary/Backup Key/Value Service</h1><p>主要分成两个部分：</p>
<ul>
<li>viewservice server，作为全局的coordinator，维护并更新当前view的情况（主要包括view number、当前primary server、当前backup server）；这里假设viewservice server永远不会崩溃。</li>
<li>Key/Value server，用来保存数据，一般情况下有两个——primary和backup，primary响应各种对数据的读写请求，backup则作为primary的副本，在primary崩溃时接管服务。</li>
</ul>
<h2 id="View-service"><a href="#View-service" class="headerlink" title="View service"></a>View service</h2><p>viewservice server给primary及backup server分别维护一个counter，周期性地increment这两个counter，同时在收到heartbeat消息时把相应的counter清零。每隔counter的大小就表示viewservice server有多久没有收到heartbeat消息了。</p>
<p>主要的功能集中在两个地方：<code>Ping()</code> RPC handler，用来接收heartbeat消息，返回当前最新的view，并为primary或者backup把counter清零；<code>tick()</code>，自身的一个计时程序，周期性地在后台执行，每次执行都增加counter值。</p>
<p>其实并没有什么特别大的坑，只要把各种细节情况考虑周全即可，包括：</p>
<ul>
<li>Primary/backup 是否存在</li>
<li>若primary/backup存在，当其中一个/两个挂掉的时候该如何更新view</li>
</ul>
<h2 id="Primary-backup-service"><a href="#Primary-backup-service" class="headerlink" title="Primary/backup service"></a>Primary/backup service</h2><p>Primary server就是个字典，保存了一些key/value pair，然后把数据在backup server上replicate了一份；它提供<code>Get(key)</code>、<code>Put(key, value)</code>，及<code>Append(key, value)</code>三类操作。这里是第一次实现不同节点间的数据同步，虽然只是两个节点。</p>
<p>主要的坑有三个：</p>
<h3 id="过滤重复的RPC"><a href="#过滤重复的RPC" class="headerlink" title="过滤重复的RPC"></a>过滤重复的RPC</h3><p>设想客户端发了一个RPC请求（例如一个<code>Append</code>请求），服务端接到请求之后完成相应的操作，然而在服务端正要通知客户端请求完成的时候网络连接挂掉了，这时客户端由于不知道之前的<code>Append</code>操作已经完成，会重新发送，因此必须把这个重复的<code>Append</code>请求过滤掉。</p>
<p>对于幂等的操作<code>Get</code>，由于它不改变数据状态，因此不需要过滤重复请求——事实上也不应过滤，因客户端仅当没收到答复时才会重新发送请求，而<code>Get</code>操作的结果就包含在答复信息中。</p>
<p>为了过滤重复的RPC，服务端需要知道客户端的当前“状态”，因此我用了一个非常粗暴的方法：每个客户端维护一个序号<code>maxSeq</code>，在发出RPC请求时附上这个序号，<strong>请求成功后</strong>就把该序号加1。同时服务端为每个客户端分别维护一个序号，记录该客户端向其发送过的最大<code>maxSeq</code>，若收到RPC请求的序号小于服务端所维护的对应<code>maxSeq</code>，则说明该请求的内容已经被执行过，服务端就可以忽略之。</p>
<h3 id="view-change"><a href="#view-change" class="headerlink" title="view change"></a>view change</h3><p>由于view只由viewservice server管理，有可能出现primary和backup持有不同view的情况。而且这种view-splitting可能在任何时候出现。</p>
<h3 id="初始化新的backup"><a href="#初始化新的backup" class="headerlink" title="初始化新的backup"></a>初始化新的backup</h3><p>当一个idle server被提升成新的backup时，需要用primary的当前状态初始化它。我实现的时候选择了push的方法：即primary在发现新的backup出现后，把自己的数据和状态推送给它。也可以用pull的方式，让新的backup主动向primary索要数据。这里需要注意的也是view-splitting的问题。</p>
<h1 id="Paxos-based-Key-Value-Service"><a href="#Paxos-based-Key-Value-Service" class="headerlink" title="Paxos-based Key/Value Service"></a>Paxos-based Key/Value Service</h1><h2 id="Preliminary"><a href="#Preliminary" class="headerlink" title="Preliminary"></a>Preliminary</h2><p>Paxos算法简单地说来就是利用quorum + two-phase commit实现的一个consensus算法。Paxos算法中没有固定的leader，因此理论上来说有很好的容错性，因为没有哪个节点会成为critical path。</p>
<p>实验中假设每个节点都维护了各自的一个log，log中的每个条目称为一个instance，每个instance有两种状态：decided和undecided。若某个节点发现自己的log中有一个undecided instance <code>i</code>，则它可以为该instance提出（propose）一个值，当系统中一半以上的节点（quorum）都在某个值<code>v</code>上达成一致，instance <code>i</code>就被decided，且具有值<code>v</code>。注意多个节点允许同时为同一个instance提出不同的值，但Paxos算法保证，当系统中一半以上的节点都正常工作且能互相通信时：</p>
<ol>
<li>某个quorum最后一定能达到concensus</li>
<li>达成一致的值<code>v</code>一定是由某个节点提出的，也即一定有某个proposer成功了</li>
<li>一旦达成一致，instance <code>i</code>的值就不再改变，即系统不可能又为该instance确定一个新的值</li>
</ol>
<p>Paxos算法包含如下几个方面：<br><figure class="highlight vhdl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">--- Paxos Proposer ---</span></span><br><span class="line"></span><br><span class="line">proposer(v):</span><br><span class="line">   <span class="keyword">while</span> <span class="keyword">not</span> decided:</span><br><span class="line">     choose n, unique <span class="keyword">and</span> higher than any n seen so far</span><br><span class="line">     send prepare(n) <span class="keyword">to</span> <span class="keyword">all</span> servers including self</span><br><span class="line">     <span class="keyword">if</span> prepare_ok(n, na, va) from majority:</span><br><span class="line">       v' = va <span class="keyword">with</span> highest na; choose own v otherwise   </span><br><span class="line">       send accept(n, v') <span class="keyword">to</span> <span class="keyword">all</span></span><br><span class="line">       <span class="keyword">if</span> accept_ok(n) from majority:</span><br><span class="line">         send decided(v') <span class="keyword">to</span> <span class="keyword">all</span></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"> <span class="comment">--- Paxos Acceptor ---</span></span><br><span class="line"></span><br><span class="line">acceptor state <span class="keyword">on</span> each node (persistent):</span><br><span class="line">  np     <span class="comment">--- highest prepare seen</span></span><br><span class="line">  na, va <span class="comment">--- highest accept seen</span></span><br><span class="line"></span><br><span class="line">acceptor<span class="symbol">'s</span> prepare(n) handler:</span><br><span class="line"> <span class="keyword">if</span> n &gt; np</span><br><span class="line">   np = n</span><br><span class="line">   reply prepare_ok(n, na, va)</span><br><span class="line"> <span class="keyword">else</span></span><br><span class="line">   reply prepare_reject</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">acceptor<span class="symbol">'s</span> accept(n, v) handler:</span><br><span class="line"> <span class="keyword">if</span> n &gt;= np</span><br><span class="line">   np = n</span><br><span class="line">   na = n</span><br><span class="line">   va = v</span><br><span class="line">   reply accept_ok(n)</span><br><span class="line"> <span class="keyword">else</span></span><br><span class="line">   reply accept_reject</span><br></pre></td></tr></table></figure></p>
<h2 id="Problems-encountered-in-implementing-Paxos"><a href="#Problems-encountered-in-implementing-Paxos" class="headerlink" title="Problems encountered in implementing Paxos"></a>Problems encountered in implementing Paxos</h2><h3 id="prepare消息中带的ballot-number-n"><a href="#prepare消息中带的ballot-number-n" class="headerlink" title="prepare消息中带的ballot number n"></a><code>prepare</code>消息中带的ballot number <code>n</code></h3><p>之前的伪代码中说只要<code>n</code>是“unique and higher than any n seen so far”即可，然而由各个节点互相之间并不知道对方会选什么样的ballot number，因此必须有某种方式来保证<strong>不同节点一定会选择不同的ballot number</strong>，否则若两个节点选择了相同的<code>n</code>，就会出现splitting brain的情况，它们无法知道到底是自己的proposal成功了还是对方的proposal成功了。</p>
<p>由于实验中所有<em>N</em>个节点被事先编号为<em>0~N-1</em>，因此我让<em>k</em>号节点使用的ballot number <code>n</code>满足$n\ \text{mod}\ N = k$，这样就保证不同节点所使用的ballot number一定不同</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// This function compute a unique ballot num for each paxos peer</span></span><br><span class="line"><span class="comment">// 1. this function works correctly IF AND ONLY IF all px.me's are distinct</span></span><br><span class="line"><span class="comment">//    and continuous, e.g. 0, 1, 2, 3, 4, 5</span></span><br><span class="line"><span class="comment">// 2. nb = min k s.t. k % px.population = px.me and k &gt; ballot</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(px *Paxos)</span> <span class="title">next_ballot</span><span class="params">(ballot <span class="keyword">int</span>)</span> <span class="title">int</span></span> &#123;</span><br><span class="line">	<span class="keyword">var</span> nb <span class="keyword">int</span></span><br><span class="line">	nb = ballot % px.population</span><br><span class="line">	nb = ballot - nb + px.me</span><br><span class="line">	<span class="keyword">if</span> nb &lt; ballot &#123;</span><br><span class="line">		nb += px.population</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> nb</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Forget-already-decided-log-instance"><a href="#Forget-already-decided-log-instance" class="headerlink" title="Forget already decided log instance"></a>Forget already decided log instance</h3><p>之前所说的一个log instance的“值”，一般是某个客户端对服务端所提出的请求，例如对于一个分布式数据库，数据被replicate到多个节点上，则这些replica需要在对数据的操作上达成一致，也即有一个全局的顺序。当两个节点的log相同时，把log中记录的操作按顺序执行（commit）一遍后，这两个节点上的数据也一定是相同的。</p>
<p>由于log不能无限增长，这里实现的Paxos库提供删除instance的功能。注意如果要删除一个instance，必须系统中的<strong>所有</strong>节点都在该instance上达成一致，且该instance中记录的操作也已经被执行了。因为最后commit某个decided instance的时间可能发生在其decided之后的任意时刻，因此Paxos库把删除log的权利移交给使用该库的应用程序，仅当该应用程序主动提出要删除某个instance时，Paxos库才会<strong>试图</strong>删除该instance（这里用“试图”是因为可能存在某个节点仍然没有commit该instance，为了保证该节点在挂掉之后能够通过系统中其它节点正确地恢复数据，其它节点需要保留该instance，因此删除instance的操作并不总能成功）。</p>
<p>在实现时，由于instance的序号是单调增加的（log是单调增长的），Paxos库提供的<code>Done(seq)</code>操作实际上做的是宣布“本节点上所有序号小于等于<code>seq</code>的log instances都可以安全的删掉了”。当在节点<em>i</em>调用<code>Done(seq)</code>时，节点<em>i</em>向所有节点广播一条<code>forget(seq)</code>消息；若某个节点收到了系统中所有节点（包括它自身）的<code>forget(seq)</code>消息，它就把自己log中序号小于等于<code>seq</code>的instance删掉。</p>
<h2 id="Problems-encountered-when-implementing-a-distributed-key-value-store-based-on-Paxos"><a href="#Problems-encountered-when-implementing-a-distributed-key-value-store-based-on-Paxos" class="headerlink" title="Problems encountered when implementing a distributed key/value store based on Paxos"></a>Problems encountered when implementing a distributed key/value store based on Paxos</h2><h3 id="Using-a-single-state-machine-to-commit-log-instances"><a href="#Using-a-single-state-machine-to-commit-log-instances" class="headerlink" title="Using a single state machine to commit log instances"></a>Using a single state machine to commit log instances</h3><p>如果客户端的请求中没有<code>Get</code>这种获取数据当前状态的请求类型，因Paxos库支持并发的propose，完全可以用多个RPC handler并发地接收多个请求，然后用Paxos库把这些请求固化到log中，而commit log instance的工作则由某个后台进程周期性的执行。然而由于<code>Get</code>操作要求获取当时最新的数据状态，其对应的log instance在decide之后必须立刻commit，否则无法返回up-to-date的数据给客户端。</p>
<p>上述原因使得propose和commit两种操作耦合起来了，在允许并发propose的情况下会把情况搞得很复杂。为了简化实现，我使用一个单独的goroutine作为state machine统领上述两种类型的操作，具体为：所有RPC handler在接到请求之后，通过一个固定的channel把请求传给state machine，state machine负责用Paxos库propose该请求、并在需要时commit log中的decided instance。由于只有一个goroutine能操作数据和状态变量，几乎不用考虑并发的问题，连锁都用不上。但是相应的，程序的性能就降低了——服务端实际上变成了一个单线程的程序。</p>
<h1 id="Sharded-Key-Value-Service"><a href="#Sharded-Key-Value-Service" class="headerlink" title="Sharded Key/Value Service"></a>Sharded Key/Value Service</h1><h2 id="Shardmaster"><a href="#Shardmaster" class="headerlink" title="Shardmaster"></a>Shardmaster</h2><ul>
<li>角色：类似之前的viewservice，只是这里面对的是多个sharding group；</li>
<li>额外功能：用Paxos实现Fault tolerance；添加了load balance的功能，可以动态的添加删除sharding group，或者把某个key在不同的sharding group之间移动。</li>
</ul>
<h3 id="A-naive-load-balance-algorithm"><a href="#A-naive-load-balance-algorithm" class="headerlink" title="A naive load balance algorithm"></a>A naive load balance algorithm</h3><p>题目要求的是：</p>
<ol>
<li>使得sharding group间最大和最小负载的差不超过1个shard；</li>
<li>在rebalance的过程中移动的shard数目最少。在满足某个条件时，存在一个很naive的算法来实现上述要求：</li>
</ol>
<ul>
<li>当知道shard的总数$N$和sharding group的总数$G$时，就可以直接算出满足上述要求的shard分布：每个sharding group所host的shard数目必然是$\lfloor N/G\rfloor$或$\lfloor N/G\rfloor + 1$。因此，在加入新的sharding group或移除某个旧的sharding group时，可以直接计算出之后的load 分布情况，并依此决定移动shard的方式。</li>
<li><p>算法如下（假设原本有$G$个sharding group）：</p>
<ul>
<li><p>rebalance after <code>Join</code>:<br>新的sharding group的期望负载是$\lfloor N/(G+1)\rfloor$。将当前系统中的各sharding group按负载大小<strong>从高到低</strong>排序，依次从中各取一个shard移到新的sharding group中，不断循环，直到让新的sharding group达到期望负载，示例如下：</p>
<figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># N=10, G=3</span></span><br><span class="line"><span class="keyword">Before </span><span class="keyword">Join: </span><span class="built_in">a1</span>=<span class="number">4</span>, <span class="built_in">a2</span>=<span class="number">3</span>, <span class="built_in">a3</span>=<span class="number">3</span></span><br><span class="line">After <span class="built_in">a4</span> <span class="keyword">joins: </span><span class="built_in">a1</span>=<span class="number">3</span>, <span class="built_in">a2</span>=<span class="number">2</span>, <span class="built_in">a3</span>=<span class="number">3</span>, <span class="built_in">a4</span>=<span class="number">2</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>rebalance after <code>Leave</code>:<br>将当前系统中的各sharding group按负载大小<strong>从低到高</strong>排序，然后从要移除的sharding group中按此顺序一个一个地将shard移交给其它group，直到把所有的shard移交完。</p>
<figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># N=10, G=4</span></span><br><span class="line"><span class="keyword">Before </span>Leave: <span class="built_in">a1</span>=<span class="number">3</span>, <span class="built_in">a2</span>=<span class="number">2</span>, <span class="built_in">a3</span>=<span class="number">3</span>, <span class="built_in">a4</span>=<span class="number">2</span></span><br><span class="line">After <span class="built_in">a4</span> leaves: <span class="built_in">a1</span>=<span class="number">3</span>, <span class="built_in">a2</span>=<span class="number">3</span>, <span class="built_in">a3</span>=<span class="number">4</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p><strong>若在某一时刻系统中的load分布满足之前的要求1</strong>，则在加入/移除某个sharding group后，通过上述算法得到的load分布仍然满足要求1，且移动的shard是最少的（即满足要求2）。因系统的初始状态（0个sharding group）是满足要求1的，故如果<strong>只有<code>Join</code>和<code>Leave</code>操作</strong>，则该算法可以保证系统的每个configuration都满足要求1，且rebalance过程移动最少的shard。注意这意味着当系统中的load分布不满足要求1时，上述算法不能保证达到负载均衡。</p>
</li>
<li>实现中碰到的坑：<code>Move</code>操作会打破负载均衡的状态，由于rebalance仅在<code>Join</code>/<code>Leave</code>后发生，算法可能会面对一个极不均衡的load分布，此时尽管算法无法使系统重新满足要求1，但可以减轻imbalance程度：例如在<code>Join</code>时，若某个existing sharding group的真实负载已经小于其期望负载，则在向新group移交shard时就跳过这个under-loaded group；<code>Leave</code>时的做法类似。</li>
</ul>
<h3 id="Load-balance-based-on-consistent-hashing"><a href="#Load-balance-based-on-consistent-hashing" class="headerlink" title="Load balance based on consistent hashing"></a>Load balance based on consistent hashing</h3><p>真实系统中使用的负载均衡算法一般基于consistent hashing，例子可以见Amazon的<a href="http://dl.acm.org/citation.cfm?id=1294281" target="_blank" rel="external">Dynamo论文</a>。由于其是基于hash的方法，无法达到作业要求的严格负载均衡（要求1），因此这里不详述。</p>
<h2 id="Sharded-Key-Value-Store"><a href="#Sharded-Key-Value-Store" class="headerlink" title="Sharded Key/Value Store"></a>Sharded Key/Value Store</h2><p>每个sharding group中的所有shard server组成一个replica group，分配给该group的shard在group中的每个server上都replicate一份。单个shard server的实现类似于之前基于Paxos的Key-Value store，额外的难点在于还要处理reconfiguration（load rebalance）的情况。</p>
<h3 id="Reconfiguration带来的坑"><a href="#Reconfiguration带来的坑" class="headerlink" title="Reconfiguration带来的坑"></a>Reconfiguration带来的坑</h3><p>这一部分的坑在于reconfiguration的时候，shard server如何处理普通的<code>Get</code>和<code>PutAppend</code>请求。很自然的做法是reconfiguration期间不接受任何其它请求，并通过Paxos把reconfiguration操作也固化到log中，从而使得整个shard group在“何时reconfiguration”这一事上达成一致。</p>
<p>由于reconfiguration会同时影响数据（某些Key会被移动）和状态（当前shard group host哪些shard），因此commit reconfiguration时也包含两个方面：其中对状态的影响应该在instance decided后<strong>立即</strong>commit，因为其会影响到对之后的所有请求的答复（例如当reconfiguration后某个key隶属的shard被移到其它group去了，那在这之后所有针对该key的请求都应该拒绝）；麻烦在于何时commit对数据的影响，因为不同的sharding group之间，对于reconfiguration的时间是没有一致性的，因此可能会出现下面几种问题：</p>
<ol>
<li>Group A发现在新的configuration中，有一些shard要从Group B移交给自己，然而Group B还没有看到这个新的configuration；</li>
<li>或Group A发现在新的configuration中，有一些shard要移交给Group B，然而Group B还没有看到这个新的configuration。</li>
<li>由于shardmaster能够通过<code>Move</code>操作在各个sharding group间任意移动shard，上述两种情况可能同时出现在Group A身上。</li>
<li>Group A中有个server $a_1$挂了好一会然后重启了，需要根据log来恢复数据状态，然而可能它挂的时候处于configuration $K_1$，而重启时Group B的configuration已经演进到了$K_2&gt;K_1+1$，此时若该server $a_1$在commit configuration $K_1+1$时需要向Group B获取shard S，那么：<ul>
<li>若shard S目前（即configuration $K_2$）不由Group B host，Group B该如何答复？</li>
<li>即使shard S目前正好由Group B host，Group B是否应该把它的当前值传递给$a_1$？要知道Group A中的其它没挂的server在commit configuration $K_1+1$时也向Group B索要过S，然而那时S对应的数据和现在所对应的数据完全可能不一样。</li>
</ul>
</li>
<li>对于“向其它group索要/移交shard”这个操作，是否有必要将其固化到Paxos的log中？</li>
</ol>
<h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p>对于上述问题，我设计了一个很低效的方法（但是能通过所有测试样例）：</p>
<ol>
<li>把每两个连续的configuration之间需要向其它group<strong>移交</strong>的数据做个快照（snapshot）存起来，这些快照按照configuration number来编号。</li>
<li>在向其它group索要shard时，附上对应的configuration number，被索要方根据configuration number返回对应的snapshot。若被索要方还没有做对应的snapshot，则拒绝请求。</li>
<li>在group之间转移shard时采用pull的方式：当commit reconfiguration请求时，若须向其它group索要shard，则当前server进入阻塞状态——重复索要直到成功获得数据为止；若只须向其它group移交数据，则仅把需要移交的数据快照保存起来，然后reconfiguration即可结束。</li>
<li>Shard server在向shardmaster查询configuration状态时，要保证查询的<strong>连续性</strong>：即若自身当前configuration number是$n$，则仅query $n+1$时的configuration。这样保证每个server都能看到所有发生的reconfiguration，从而保证数据快照的正确性。</li>
<li>每个shard server专门提供一个RPC handler用于向其它group<strong>移交</strong>shard，由于移交的都是快照，而<strong>快照一旦生成就不会再改变</strong>，因此该操作只需判断所需的快照是否存在，除此之外不会影响server的数据和状态，故没必要使用Paxos将该操作固化在log中。</li>
</ol>
<h1 id="Persistence"><a href="#Persistence" class="headerlink" title="Persistence"></a>Persistence</h1><p>这部分主要是让你给上一节的Sharded Key-Value Server增加failure recovery的功能：把数据和log保存在硬盘上，这样当节点重启后能够快速地恢复到崩溃前的状态。这个作业的主要难点在于需要persist的东西包括四种：server的数据、server的状态、Paxos的log、Paxos的状态。这四样东西直接必须相互consistent，否则重启后恢复出来的东西就有问题。其中Paxos作为一个底层库，按理来说其对自身log和状态的persistence不应该暴露给上层应用，作业中给的提示其实也在暗示这样做：每个Key单独存成一个文件，这样每commit一个log instance的同时就修改对应Key在硬盘上的文件。然而Key一多就会生成好多个文件，我觉得这样太傻逼了，就决心把所有的数据存在一个文件里，结果后来证明自己才是真正的傻逼。</p>
<p>这个作业远比我一开始想象的更难。在实现过程中，Google那篇关于Chubby的论文<a href="http://dl.acm.org/citation.cfm?id=1281103" target="_blank" rel="external">Paxos Made Live - An Engineering Perspective</a>给了我很大帮助。这种工业界的论文如果只是简单地看一遍的话，里面很多有价值的东西根本体会不到，只有当你做一个类似的系统/功能的时候，再回头看才会觉得“啊真漂亮”。</p>
<h2 id="Basic-ideas"><a href="#Basic-ideas" class="headerlink" title="Basic ideas"></a>Basic ideas</h2><p>之前提到我打算把一个server的所有key-value数据都存在一个文件里，这个决定直接影响了后续的整个实现方式。对于正在工作的shard server，数据（也就是Key/Value pair）是以字典的形式存在内存里的，而字典是没有顺序的。如果在每commit一个log instance后，就修改相应key在硬盘上的内容，为了在那个数据文件里找到这个key的位置，将不得不把整个文件扫一遍——这个光是听起来就很傻，所以我做了第二个重要的设计决定：用一个后台进程定期地给整个数据库做快照（snapshot），然后用新的快照代替老的快照。由于快照过程和底层工作的Paxos进程是独立的，为了保证二者的一致性，我采取了Google论文里描述的方法：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// acquire lock to freeze the server</span></span><br><span class="line">kv.state_mu.Lock()</span><br><span class="line"></span><br><span class="line"><span class="comment">// ask paxos for snapshot handler</span></span><br><span class="line">px_state := kv.px.CaptureState()</span><br><span class="line"></span><br><span class="line"><span class="comment">// take snapshot of current server state</span></span><br><span class="line">kv_state := kv.CaptureState()</span><br><span class="line"></span><br><span class="line"><span class="comment">// store snapshot on the disk</span></span><br><span class="line"><span class="keyword">if</span> err := kv.save_state(&amp;kv_state, &amp;px_state); err != <span class="literal">nil</span>&#123;</span><br><span class="line">	<span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// tell paxos to eleminate log before the snapshotted point</span></span><br><span class="line"><span class="keyword">if</span> err := kv.px.Truncate(kv_state.CommittedPoint); err != <span class="literal">nil</span> &#123;</span><br><span class="line">	<span class="built_in">panic</span>(err)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// snapshot finished, release lock</span></span><br><span class="line">kv.state_mu.Unlock()</span><br></pre></td></tr></table></figure>
<p>说明</p>
<ol>
<li><p>快照和Paxos log之间必须保持consistent，因此server在制作快照时，必须知道所提取的快照在log中的相对位置。故在制作快照的第一步，server先向底层的Paxos库索取一个snapshot handler（实际上就是让Paxos给自己的状态拍一个快照，主要是此时对应的最大commited instance number，也即当前快照对应的instance在log中的位置），这个snapshot handler将一并保存在server的快照中。在server从崩溃中恢复时，它利用这个snapshot handler确定自己在paxos log中的位置。</p>
</li>
<li><p>由于Paxos自身也会将log写在硬盘上，当server的快照完成之后，其对应的时间点之前的log文件其实就可以删掉了。因此server可以显式地要求Paxos清理无用的log文件（这有点类似之前的<code>Paxos.Done</code>功能，只不过那时是释放内存空间，现在是释放硬盘空间）。</p>
</li>
<li><p>在某个server重启并试图恢复状态时，其可能需要向其余server索取某些已经不存在的log内容，这时由于这些log已经从硬盘上删掉了，没人能帮助它恢复。此时这个重启的server应该向其余server索要它们的快照，然后用这些快照重新初始化自己的数据和状态。</p>
</li>
<li><p>server的快照和Paxos的log是分开保存的，在重启时也要分别初始化这二者。</p>
</li>
</ol>
<h2 id="Modification-to-Paxos-library"><a href="#Modification-to-Paxos-library" class="headerlink" title="Modification to Paxos library"></a>Modification to Paxos library</h2><p>需要为Paxos库增加两个功能：</p>
<ol>
<li><p><code>save_log_entry(seq)</code>：把序号为<code>seq</code>的log instance写到磁盘上。这里需要考虑的问题是当一个instance处于何种状态的时候才能把它写到磁盘上：肯定不是undecided的时候，那应该是decided还是commited之后呢？由于当一个log instance被decided之后，服务端就可以答复客户端了（这里是针对<code>PutAppend</code>操作而言），此时在外界看来这个log instance里包含的请求就已经完成了，故为了保证这种对外的一致性在server重启后能得到恢复，一旦一个instance被decide，Paxos就立刻把它写到硬盘上。</p>
</li>
<li><p><code>Truncate(seq)</code>：上层的server application可以显示地调用这个函数来要求Paxos清理序号小于<code>seq</code>的log文件。注意这里不同于<code>Paxos.Done</code>，<code>Paxos.Truncate</code>一旦被调用，就一定会释放自己硬盘上的空间，这个过程中不会考虑其它节点的情况。此外，它也不考虑这个instance是否还存在于内存中——因此完全可能出现这种情况：某个log instance在硬盘上的记录已经被清理了，但它还存在于内存中（即还没有被<code>Forget</code>清理）。</p>
</li>
</ol>
<h2 id="Problems-encountered"><a href="#Problems-encountered" class="headerlink" title="Problems encountered"></a>Problems encountered</h2><p>主要的问题都发生在崩溃恢复阶段：</p>
<ol>
<li>server A通过自己硬盘上的快照恢复之后，需要向同group的其它peer server获取在其崩溃期间产生的decided log instance来把自己的状态更新到与其它server一致，若其它server 在A崩溃的这段时间里一直正常工作，则它们的<strong>内存</strong>中会保有A需要的全部log内容，这是由下述关系所保证的：<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A.maxDecided &gt;= A.committedPoint &gt;= others.globalMinUnForgotten</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>这是最好的情况，A可以直接通过不断地propose empty value来获取其它server的decided log instance，从而逐渐地补全自己的log。</p>
<ol>
<li><p>若在server A崩溃的那段时间里，其它的server也都经历了崩溃，这使得A想要补齐的log instance可能已经永远丢失了——内存里的都没了，硬盘上的也被truncate掉了。这时server A可以判断出自己的状态和数据已经过于落后了，因此A应该向其它server索要它们的server snapshot及paxos log snapshot，然后用这些新的snapshot替换自身的snapshot，并重启server及paxos。</p>
</li>
<li><p>如果server A在参与关于log instance <em>K</em>的某轮Paxos proposal的过程中崩溃了，那当A重启后，它不能再次参与任何关于instance <em>K</em>的proposal：因为A在崩溃前可能已经向某个server做出了<code>Promise</code>或<code>AcceptOK</code>，然而在重启后A无法知道自己曾经做出过的答复（Paxos不会把undecided instance写到硬盘），这时如果A又参与同一轮proposal，则其可能会错误地向另一个server做出<code>Promise</code>或<code>AcceptOK</code>，即“A incorrectly change its mind”，这会彻底破坏Paxos算法的结果，使得整个系统失去一致性。<br>然而，由于A不可能通过快照获知自己在崩溃前是否参与了某个proposal，其必须通过某种方法从其它server处获取该信息。<br>为了解决上述问题，我让每个server维护一个变量<code>highestSeqSeen</code>，保存其见过的最大的log instance seq number（no matter whether it is decided or not），当A重启后，</p>
<ul>
<li>其首先令自身的Paxos进入一个“Freezed”状态，在该状态下，Paxos忽略所有<code>Prepare</code>和<code>Accept</code>请求；</li>
<li>然后，A向group中的quorum获取它们的<code>highestSeqSeen</code>，取其中的最大值$H$；</li>
<li>接下来，A不断地propose empty instance，直到把序号小于等于$H$的log instance都给decide；</li>
<li>最后，A给自身的Paxos解除“Freezed”状态，重新开始正常工作。</li>
</ul>
</li>
</ol>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p><em>我不想写总结了，这篇文章只是写给自己看的笔记而已为什么一定要写总结好烦啊啊啊啊啊</em></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/03/26/mit-distributed-system-course-summary/" data-id="cipnq4nkv00163j4kgxnsh2an" class="article-share-link">Share</a>
      
        <a href="http://yoursite.com/2016/03/26/mit-distributed-system-course-summary/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/distributed-system/">distributed-system</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/programming/">programming</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2016/03/21/Python-generators-coroutines-and-asyncio-library-2/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Python coroutines and asyncio library (2)</div>
    </a>
  
</nav>

  
</article>


<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>
</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Math/">Math</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/programming/">programming</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Emacs/">Emacs</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux-Kernel/">Linux Kernel</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/distributed-system/">distributed-system</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/math/">math</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/programming/">programming</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/puzzle/">puzzle</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/">python</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Emacs/" style="font-size: 10px;">Emacs</a> <a href="/tags/Linux-Kernel/" style="font-size: 20px;">Linux Kernel</a> <a href="/tags/distributed-system/" style="font-size: 10px;">distributed-system</a> <a href="/tags/math/" style="font-size: 10px;">math</a> <a href="/tags/programming/" style="font-size: 10px;">programming</a> <a href="/tags/puzzle/" style="font-size: 10px;">puzzle</a> <a href="/tags/python/" style="font-size: 20px;">python</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/03/">March 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/02/">February 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2016/03/26/mit-distributed-system-course-summary/">Summary for MIT 6.824- Course projects</a>
          </li>
        
          <li>
            <a href="/2016/03/21/Python-generators-coroutines-and-asyncio-library-2/">Python coroutines and asyncio library (2)</a>
          </li>
        
          <li>
            <a href="/2016/03/20/Python-generators-coroutines-and-asyncio-library-1/">Python coroutines and asyncio library (1)</a>
          </li>
        
          <li>
            <a href="/2016/02/22/A-Superhard-Elementary-Math-Problem/">A Superhard Elementary Math Problem</a>
          </li>
        
          <li>
            <a href="/2016/02/20/numpy-with-cython/">Numpy with Cython</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2016 xyguo<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
    <a href="/links" class="mobile-nav-link">Links</a>
  
</nav>
    
<script>
  var disqus_shortname = 'xyguo';
  
  var disqus_url = 'http://yoursite.com/2016/03/26/mit-distributed-system-course-summary/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>




  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>